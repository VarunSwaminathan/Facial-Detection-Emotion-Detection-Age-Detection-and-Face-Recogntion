# -*- coding: utf-8 -*-
"""face_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gDns-jKjFhdWkryr20caEvBtsfyToj9O
"""

import pandas as pd
import cv2 as cv
import numpy as np

from google.colab import files
uploaded = files.upload()
image = cv.imread('count_faces.jpg')

from google.colab.patches import cv2_imshow

faceDetect = cv.CascadeClassifier(cv.data.haarcascades  + "haarcascade_frontalface_default.xml")
eye_cascade = cv.CascadeClassifier(cv.data.haarcascades + 'haarcascade_eye.xml')

gray_image = cv.cvtColor(image, cv.COLOR_BGR2GRAY)

faces = faceDetect.detectMultiScale(gray_image, 1.2, 5)

for (x, y, w, h) in faces:
    cv.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)

cv2_imshow(gray_image)
cv.waitKey(0)

num_faces = len(faces)
print(num_faces)

from PIL import Image
import dlib
import keras
from keras.models import load_model
import cv2

from google.colab import files
import tensorflow as tf
uploaded = files.upload()
model = cv2.dnn.readNetFromCaffe('deploy_gender.prototxt', 'gender_net.caffemodel')

def detect_emotions(image):
    # Preprocess the image
    image = cv2.resize(image, (48, 48))
    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    image = np.expand_dims(image, axis=-1)
    image = np.expand_dims(image, axis=0)

    # Predict the emotions
    predictions = model.predict(image)
    emotions = ['Anger', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']
    emotion_index = np.argmax(predictions)
    emotion = emotions[emotion_index]

    # Return the emotion
    return emotion

faces = faceDetect.detectMultiScale(gray_image, 1.2, 5)

for (x, y, w, h) in faces:
    # Extract the face region
    face_image = image[y:y+h, x:x+w]

    # Detect emotions from the face image
    emotion = detect_emotions(image)

    # Draw the emotion label on the image
    cv2.putText(image, emotion, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)

cv2_imshow(faces)
cv2.waitKey(0)
cv2.destroyAllWindows()

for (x, y, w, h) in faces:
    # Extract face ROI
    roi = image[y:y+h, x:x+w]
    
    # Preprocess face ROI for gender recognition model
    blob = cv2.dnn.blobFromImage(cv2.resize(roi, (227, 227)), 1.0, (227, 227), (78.4263377603, 87.7689143744, 114.895847746), swapRB=False)

    # Set input to gender recognition model and perform forward pass
    gender_model.setInput(blob)
    preds = gender_model.forward()

    # Get predicted gender
    gender = 'Male' if preds[0][0] > preds[0][1] else 'Female'
    
    # Draw bounding box and label for face
    cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)
    cv2.putText(image, gender, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

# Display image
cv2.imshow('image', image)
cv2.waitKey(0)
cv2.destroyAllWindows()

